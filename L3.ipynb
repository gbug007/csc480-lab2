{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2d8b3187",
      "metadata": {
        "id": "2d8b3187"
      },
      "source": [
        "# CSC 480-F25 Lab 3: Agentic Heuristic Search (NYT Spelling Bee)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "597fcded",
      "metadata": {
        "id": "597fcded"
      },
      "source": [
        "# Authors:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc34f35",
      "metadata": {
        "id": "ecc34f35"
      },
      "source": [
        "***Grady Schneider***\n",
        "\n",
        "California Polytechnic State University, San Luis Obispo;\n",
        "\n",
        "Computer Science & Software Engineering Department"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d17928c",
      "metadata": {
        "id": "5d17928c"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This lab focuses on:\n",
        "- Integrating a provided generalized search engine as a tool inside an agentic workflow\n",
        "- Designing an agentic heuristic system that collaborates to estimate $h(n)$ for NYT Spelling Bee states\n",
        "- Implementing a custom cost function $g(n)$ and evaluating search strategies (Uniform Cost, A*)\n",
        "- Coordinating agent communication via MCP-style tool exposure and A2A interactions\n",
        "- Reflecting on how agentic heuristics complement classical search methods\n",
        "\n",
        "NOTE: The Spelling Bee problem definition and generalized search function are provided for you. Your primary work is to wire them into your agentic solution and iterate on the heuristic design (see part 2 of this notebook).\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "- Integrate a provided generalized search function as a tool within an AutoGen-based agentic system\n",
        "- Design and implement an agent team that produces numeric heuristic estimates to guide search\n",
        "- Define and justify a cost function that complements your heuristic in the Spelling Bee domain\n",
        "- Specify MCP-style tool schemas and A2A message flows for heuristic collaboration\n",
        "- Analyze how different heuristic strategies impact search quality, cost, and convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c55e9e0d",
      "metadata": {
        "id": "c55e9e0d"
      },
      "source": [
        "# Part 1: Agentic Heuristic Design and Planning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168872dd",
      "metadata": {
        "id": "168872dd"
      },
      "source": [
        "## 1. Problem Statement & Search Context\n",
        "\n",
        "**Provided Problem:** NYT Spelling Bee puzzle instance (letters, required center letter, dictionary utilities)\n",
        "\n",
        "**Goal:** Integrate the provided Spelling Bee problem specification with the generalized search engine and your agentic heuristic.\n",
        "\n",
        "**Task Breakdown:** Outline the high-level steps you will take to reach a working solution.\n",
        "1. Determine problem constraints and the problem space I'll be working with. See which tools I'll need to learn and read steps in advanced.\n",
        "2. Expose the parameters in the generalized search tool necessary for my agents. Don't need to expose every inner working detail, just the parts important to the agents.\n",
        "3. I'll think about how an intelligent agent could efficiently break down and pass off tasks to another agent. Each agent probably shouldn't have a lot of duties but also doesn't need to be overly basic either. I'll run them and check the output in Colab.\n",
        "4. I'll test that the agent reaches an expected and/or reasonable conclusion, and if the heuristic is off or some part of the output is incorrect I'll figure out a way to make it more accurate, likely by modifying the agents or what the generalized search tool exposes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56161440",
      "metadata": {
        "id": "56161440"
      },
      "source": [
        "## 2. Agentic Heuristic Team Definition\n",
        "\n",
        "Define the agents who collaborate to estimate $h(n)$ for a given Spelling Bee state. Feel free to use more, or fewer.\n",
        "\n",
        "### Agent 1: ScoringAgent\n",
        "- **Role:** Determines potential achievable score according to Spelling Bee scoring metrics\n",
        "- **Responsibilities:** Create score based on given scoring metrics\n",
        "- **Inputs:** Partial word state\n",
        "- **Outputs:** Score (higher is better) normalized to a 100-point scale, using 100 for a high-scoring word (around 10 points)\n",
        "- **Success Criteria:** The words determined by the algorithm are high-scoring\n",
        "\n",
        "### Agent 2: CompletionEffortAgent\n",
        "- **Role:** Determines the amount of effort to create a complete word\n",
        "- **Responsibilities:** Create a score on a 100-point scale, using 100 if it is lower effort to achieve the goal.\n",
        "- **Inputs:** Partial word state\n",
        "- **Outputs:** Score (higher is better)\n",
        "- **Success Criteria:** Agent gives high score to words that are easier to be completed\n",
        "\n",
        "### Agent 3: LikelyToFormAgent\n",
        "- **Role:** Determines how likely agent is to form an actual valid word or multiple valid words\n",
        "- **Responsibilities:** Create a score on a 100-point scale based on the likelihood of the partial state forming a valid word\n",
        "- **Inputs:** Partial word state\n",
        "- **Outputs:** Score (higher is better)\n",
        "- **Success Criteria:** Agent gives higher score to words that are very likely to form valid word(s)\n",
        "\n",
        "### Agent 4: ScoreCombiner\n",
        "- **Role:** Consider score of other agents to create a combined score\n",
        "- **Responsibilities:** Makes the best combined score given information from the other 3 agents\n",
        "- **Inputs:** Scores from each agent\n",
        "- **Outputs:** Combined score\n",
        "- **Success Criteria:** Combined score results in better results than the individual agents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53934c12",
      "metadata": {
        "id": "53934c12"
      },
      "source": [
        "## 3. Tool Integration & Coordination Pattern\n",
        "\n",
        "**Chosen Pattern:** Manager-Worker\n",
        "\n",
        "**Justification:**\n",
        "The first three agents work separately to create individual scores, and the final agent considers their scores and forms a combined heuristic score that considers all the important factors. They take states from the generalized search tool and generate scores which can be used as part of an informed search algorithm.\n",
        "\n",
        "**Integration Plan:** Outline how you will connect the provided components.\n",
        "- Register the generalized_search function as a tool (inputs/outputs, MCP schema)\n",
        "- Connect `cost_fn` (pure Python) and `heuristic_fn` (agentic) to the tool call\n",
        "- Describe any state or configuration the orchestrator agent must maintain\n",
        "- Identify how heuristic scores will be aggregated into a single numeric value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdec6d3d",
      "metadata": {
        "id": "cdec6d3d"
      },
      "source": [
        "## 4. Communication Design\n",
        "\n",
        "### Model Context Protocol (MCP)\n",
        "**Tool Schema:** I'm going to expose the problem itself (function pointer), the cost function (function pointer), the heuristic function (function pointer), and the strategy (string) to the search function. I'm going to expose the results including state to the agents.\n",
        "\n",
        "### Agent-to-Agent (A2A) Interactions\n",
        "\n",
        "Describe the kind of communications you expect from your system.\n",
        "\n",
        "- **Purpose:** [Why this message is sent]\n",
        "- **Key Fields:** [Scores, rationales, state descriptors]\n",
        "- **Message Format:** [Short description of structure]\n",
        "\n",
        "#### Interaction 1: ScoringAgent → ScoreCombiner\n",
        "- **Purpose:** This message is sent so that the score combiner can receive more information.\n",
        "- **Key Fields:** Score as calculated by the scoring agent\n",
        "- **Message Format:** \"ScoringAgent: [score]\"\n",
        "\n",
        "#### Interaction 2: CompletionEffortAgent → ScoreCombiner\n",
        "- **Purpose:** This message is sent so that the score combiner can receive more information.\n",
        "- **Key Fields:** Score as calculated by the completion effort agent\n",
        "- **Message Format:** \"CompletionEffortAgent: [score]\"\n",
        "\n",
        "#### Interaction 3: LikelyToFormAgent → ScoreCombiner\n",
        "- **Purpose:** This message is sent so that the score combiner can receive more information.\n",
        "- **Key Fields:** Score as calculated by the likely to form agent\n",
        "- **Message Format:** \"LikelyToFormAgent: [score]\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eacb6914",
      "metadata": {
        "id": "eacb6914"
      },
      "source": [
        "# Part 2: Integrating the Generalized Search Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c1fd14e",
      "metadata": {
        "id": "2c1fd14e"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Install required packages and configure model access before running the agentic heuristic experiments."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\"\"\"Utility classes and helpers for the agentic Spelling Bee lab.\n",
        "\n",
        "This module implements a ``SpellingBeeProblem`` class that models the New\n",
        "York Times Spelling Bee puzzle as a search problem.  It is designed to be\n",
        "consumed by the generalized search engine referenced in ``L3.ipynb`` and\n",
        "exposes a minimal API for generating successor states, evaluating goal\n",
        "states, and enumerating valid target words.\n",
        "\n",
        "The implementation focuses on pedagogy: it keeps the surface area small and\n",
        "easy to reason about while still being faithful to the official game rules:\n",
        "\n",
        "* Words must contain only the provided letters.\n",
        "* The required (\"center\") letter must appear at least once.\n",
        "* Words must meet a configurable minimum length (default: 4).\n",
        "* Repeated letters are allowed.\n",
        "\n",
        "To keep the branching factor manageable for search, we pre-filter a word list\n",
        "and retain only entries that are feasible under the given rules.  We also\n",
        "precompute all valid prefixes so that the successor generator can prune branches\n",
        "that can no longer lead to a valid solution.\n",
        "\n",
        "Example\n",
        "-------\n",
        "\n",
        ">>> problem = SpellingBeeProblem.from_letters(\n",
        "...     letters=[\"A\", \"D\", \"E\", \"L\", \"O\", \"P\", \"R\"],\n",
        "...     required_letter=\"O\",\n",
        "... )\n",
        ">>> problem.successors(\"\")[:3]\n",
        "[(\"A\", \"A\"), (\"D\", \"D\"), (\"E\", \"E\")]\n",
        ">>> problem.is_goal(\"PAROLED\")\n",
        "True\n",
        "\n",
        "The class will attempt to load a dictionary from one of several common\n",
        "locations (``/usr/share/dict/words`` first) but also ships with a tiny fallback\n",
        "list so the API remains usable out-of-the-box.  For serious experimentation,\n",
        "point ``dictionary_path`` at a richer corpus.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import asyncio\n",
        "import heapq\n",
        "import inspect\n",
        "from dataclasses import dataclass, field\n",
        "from itertools import count\n",
        "from pathlib import Path\n",
        "from typing import Callable, Dict, Iterable, List, Optional, Sequence, Set, Tuple, Union\n",
        "\n",
        "__all__ = [\"SpellingBeeProblem\", \"SearchResult\", \"generalized_search\"]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Dictionary helpers\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "_FALLBACK_WORDS: Set[str] = {\n",
        "    # pangrams using common Spelling Bee letter sets\n",
        "    \"LEOPARD\",\n",
        "    \"PAROLED\",\n",
        "    \"PARADOLE\",  # archaic but keeps prefixes alive\n",
        "    \"TRIANGLE\",\n",
        "    \"ALTERING\",\n",
        "    \"RELATING\",\n",
        "    \"INTEGRAL\",\n",
        "    \"DEALER\",\n",
        "    \"LOADER\",\n",
        "    \"PEDAL\",\n",
        "    \"PAROLED\",\n",
        "    \"PARLOR\",\n",
        "    \"PALADIN\",\n",
        "    \"RENOVATED\",\n",
        "    \"RATIONED\",\n",
        "    \"RATION\",\n",
        "    \"RATIONING\",\n",
        "    \"RATIONED\",\n",
        "    \"RATIONER\",\n",
        "    \"RATIONERS\",\n",
        "    # shorter words for generic coverage\n",
        "    \"ROAD\",\n",
        "    \"READ\",\n",
        "    \"LEAD\",\n",
        "    \"PALE\",\n",
        "    \"REAL\",\n",
        "    \"DEAL\",\n",
        "    \"RAIL\",\n",
        "    \"TRAIL\",\n",
        "    \"LATER\",\n",
        "    \"ALERT\",\n",
        "    \"ALTER\",\n",
        "    \"TREAD\",\n",
        "    \"TREADLE\",\n",
        "    \"PETAL\",\n",
        "    \"LEAPT\",\n",
        "    \"LEAPT\",\n",
        "    \"RENT\",\n",
        "    \"TONE\",\n",
        "    \"NOTE\",\n",
        "    \"TREAT\",\n",
        "    \"PLATE\",\n",
        "    \"PLEAT\",\n",
        "    \"LEAPT\",\n",
        "    \"OPAL\",\n",
        "    \"POET\",\n",
        "    \"REAP\",\n",
        "    \"ROPE\",\n",
        "    \"LOOP\",\n",
        "    \"LOOPS\",\n",
        "    \"ROOT\",\n",
        "    \"ROOTED\",\n",
        "}\n",
        "\n",
        "\n",
        "def _load_word_list(\n",
        "    dictionary: Optional[Iterable[str]] = None, dictionary_path: Optional[Path] = None\n",
        ") -> Set[str]:\n",
        "    \"\"\"Load a candidate dictionary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dictionary:\n",
        "            Optional iterable of words supplied programmatically.\n",
        "    dictionary_path:\n",
        "            Optional file path containing one word per line.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    set of uppercase words without surrounding whitespace.\n",
        "    \"\"\"\n",
        "\n",
        "    if dictionary is not None:\n",
        "        return {str(word).strip().upper() for word in dictionary if word}\n",
        "\n",
        "    if dictionary_path is not None:\n",
        "        path = Path(dictionary_path)\n",
        "        if not path.is_file():\n",
        "            raise FileNotFoundError(f\"Dictionary path does not exist: {path}\")\n",
        "        with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
        "            return {line.strip().upper() for line in handle if line.strip()}\n",
        "\n",
        "    default_candidates: Tuple[Path, ...] = (\n",
        "        Path(\"/usr/share/dict/words\"),\n",
        "        Path(\"/usr/share/dict/web2\"),\n",
        "        Path(__file__).with_name(\"words.txt\"),\n",
        "    )\n",
        "\n",
        "    for candidate in default_candidates:\n",
        "        if candidate.is_file():\n",
        "            with candidate.open(\"r\", encoding=\"utf-8\") as handle:\n",
        "                return {line.strip().upper() for line in handle if line.strip()}\n",
        "\n",
        "    # Nothing else available – fall back to the bundled miniature list.\n",
        "    return set(_FALLBACK_WORDS)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Core problem representation\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class SearchResultMetadata:\n",
        "    \"\"\"Small helper container for search-related metadata.\n",
        "\n",
        "    The generalized search implementation provided to students may choose to\n",
        "    stash additional metadata alongside each state (for example, whether the\n",
        "    word is a pangram). Exposing a typed container keeps the interface tidy\n",
        "    without committing us to a concrete implementation in this lab.  The\n",
        "    ``SpellingBeeProblem`` currently sets this to ``None`` in successor tuples,\n",
        "    but the attribute is defined for completeness and future extension.\n",
        "    \"\"\"\n",
        "\n",
        "    is_pangram: bool = False\n",
        "    score: int = 0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SearchNode:\n",
        "    \"\"\"Represents a node in the search frontier.\"\"\"\n",
        "\n",
        "    state: str\n",
        "    parent: Optional[\"SearchNode\"]\n",
        "    action: Optional[str]\n",
        "    path_cost: float\n",
        "    heuristic: float = 0.0\n",
        "    metadata: Optional[SearchResultMetadata] = None\n",
        "\n",
        "    def total_cost(self) -> float:\n",
        "        return self.path_cost + self.heuristic\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    \"\"\"Return object produced by ``generalized_search``.\"\"\"\n",
        "\n",
        "    success: bool\n",
        "    goal_state: Optional[str]\n",
        "    actions: List[str]\n",
        "    cost: float\n",
        "    expansions: int\n",
        "    explored: int\n",
        "    frontier_size: int\n",
        "\n",
        "\n",
        "class SpellingBeeProblem:\n",
        "    \"\"\"Represents a single instance of the NYT Spelling Bee puzzle.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    letters:\n",
        "            An iterable containing the seven permitted letters.\n",
        "    required_letter:\n",
        "            The mandatory letter that must appear in every valid solution.\n",
        "    dictionary:\n",
        "            Optional iterable of candidate words.  If provided, overrides\n",
        "            ``dictionary_path`` and the default loaders.\n",
        "    dictionary_path:\n",
        "            Optional path to a newline-delimited word list.\n",
        "    min_word_length:\n",
        "            Minimum allowed word length. The official Spelling Bee uses 4.\n",
        "    \"\"\"\n",
        "\n",
        "    DEFAULT_MIN_WORD_LENGTH = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        letters: Iterable[str],\n",
        "        required_letter: str,\n",
        "        *,\n",
        "        dictionary: Optional[Iterable[str]] = None,\n",
        "        dictionary_path: Optional[Path | str] = None,\n",
        "        min_word_length: int = DEFAULT_MIN_WORD_LENGTH,\n",
        "    ) -> None:\n",
        "        normalized_letters = tuple(self._normalize_letter(letter) for letter in letters)\n",
        "        if len(normalized_letters) == 0:\n",
        "            raise ValueError(\"At least one letter must be provided\")\n",
        "\n",
        "        if len(set(normalized_letters)) != len(normalized_letters):\n",
        "            # Duplicates are technically allowed in the daily puzzle, but we keep\n",
        "            # the canonical set unique to simplify reasoning.\n",
        "            normalized_letters = tuple(dict.fromkeys(normalized_letters))\n",
        "\n",
        "        normalized_required = self._normalize_letter(required_letter)\n",
        "        if normalized_required not in normalized_letters:\n",
        "            raise ValueError(\n",
        "                \"Required letter must be included in the provided letter set\"\n",
        "            )\n",
        "\n",
        "        if min_word_length < 1:\n",
        "            raise ValueError(\"Minimum word length must be positive\")\n",
        "\n",
        "        self._letters: Tuple[str, ...] = normalized_letters\n",
        "        self._letter_set: Set[str] = set(normalized_letters)\n",
        "        self._required_letter: str = normalized_required\n",
        "        self._min_word_length: int = min_word_length\n",
        "\n",
        "        if isinstance(dictionary_path, str):\n",
        "            dictionary_path = Path(dictionary_path)\n",
        "\n",
        "        raw_dictionary = _load_word_list(\n",
        "            dictionary=dictionary, dictionary_path=dictionary_path\n",
        "        )\n",
        "        if not raw_dictionary:\n",
        "            raise ValueError(\"Dictionary must contain at least one word\")\n",
        "\n",
        "        self._valid_words: Set[str] = {\n",
        "            word for word in raw_dictionary if self._is_candidate_word(word)\n",
        "        }\n",
        "\n",
        "        if not self._valid_words:\n",
        "            raise ValueError(\n",
        "                \"No valid words remain after filtering, consider supplying a larger dictionary\"\n",
        "            )\n",
        "\n",
        "        self._max_word_length: int = max(len(word) for word in self._valid_words)\n",
        "        self._prefixes: Set[str] = self._build_prefix_set(self._valid_words)\n",
        "        self._pangrams: Set[str] = {\n",
        "            word for word in self._valid_words if self.is_pangram(word)\n",
        "        }\n",
        "\n",
        "        # Include the empty string so the very first expansion is permitted.\n",
        "        self._prefixes.add(\"\")\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Public constructor helpers\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    @classmethod\n",
        "    def from_letters(\n",
        "        cls,\n",
        "        letters: Sequence[str],\n",
        "        required_letter: str,\n",
        "        *,\n",
        "        dictionary: Optional[Iterable[str]] = None,\n",
        "        dictionary_path: Optional[Path | str] = None,\n",
        "        min_word_length: int = DEFAULT_MIN_WORD_LENGTH,\n",
        "    ) -> \"SpellingBeeProblem\":\n",
        "        \"\"\"Factory matching the usage pattern in the lab notebook.\"\"\"\n",
        "\n",
        "        return cls(\n",
        "            letters=letters,\n",
        "            required_letter=required_letter,\n",
        "            dictionary=dictionary,\n",
        "            dictionary_path=dictionary_path,\n",
        "            min_word_length=min_word_length,\n",
        "        )\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Search interface expected by the generalized search tool\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    @property\n",
        "    def letters(self) -> Tuple[str, ...]:\n",
        "        \"\"\"Return the tuple of allowed letters (ordered as supplied).\"\"\"\n",
        "\n",
        "        return self._letters\n",
        "\n",
        "    @property\n",
        "    def required_letter(self) -> str:\n",
        "        \"\"\"Return the mandatory letter for all valid words.\"\"\"\n",
        "\n",
        "        return self._required_letter\n",
        "\n",
        "    @property\n",
        "    def min_word_length(self) -> int:\n",
        "        return self._min_word_length\n",
        "\n",
        "    @property\n",
        "    def valid_words(self) -> Set[str]:\n",
        "        return set(self._valid_words)\n",
        "\n",
        "    @property\n",
        "    def pangrams(self) -> Set[str]:\n",
        "        return set(self._pangrams)\n",
        "\n",
        "    @property\n",
        "    def max_word_length(self) -> int:\n",
        "        return self._max_word_length\n",
        "\n",
        "    def initial_state(self) -> str:\n",
        "        \"\"\"Return the initial (empty) state for search.\"\"\"\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def successors(\n",
        "        self, state: str\n",
        "    ) -> List[Tuple[str, str, Optional[SearchResultMetadata]]]:\n",
        "        \"\"\"Expand the given state by appending each feasible letter.\n",
        "\n",
        "        Returns a list of ``(action, next_state, metadata)`` tuples, mirroring\n",
        "        the pattern often used in search textbooks.  The metadata element is\n",
        "        optional - we currently return ``None`` to keep things simple - but the\n",
        "        slot is present so students can extend it without modifying the search\n",
        "        engine.\n",
        "        \"\"\"\n",
        "\n",
        "        normalized_state = self._normalize_word(state)\n",
        "        if len(normalized_state) >= self._max_word_length:\n",
        "            return []\n",
        "\n",
        "        successors: List[Tuple[str, str, Optional[SearchResultMetadata]]] = []\n",
        "        for letter in self._letters:\n",
        "            next_state = normalized_state + letter\n",
        "            if next_state in self._prefixes:\n",
        "                successors.append((letter, next_state, None))\n",
        "        return successors\n",
        "\n",
        "    def is_goal(self, state: str) -> bool:\n",
        "        \"\"\"Check whether ``state`` constitutes a goal word under puzzle rules.\"\"\"\n",
        "\n",
        "        normalized_state = self._normalize_word(state)\n",
        "        return normalized_state in self._valid_words\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Validation helpers\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def contains_only_allowed_letters(self, word: str) -> bool:\n",
        "        \"\"\"Return ``True`` if *word* is comprised solely of permitted letters.\"\"\"\n",
        "\n",
        "        normalized = self._normalize_word(word)\n",
        "        return bool(normalized) and set(normalized).issubset(self._letter_set)\n",
        "\n",
        "    def contains_required_letter(self, word: str) -> bool:\n",
        "        \"\"\"Return ``True`` if *word* includes the required letter.\"\"\"\n",
        "\n",
        "        return self._required_letter in self._normalize_word(word)\n",
        "\n",
        "    def is_valid_word(self, word: str) -> bool:\n",
        "        \"\"\"Return ``True`` if *word* satisfies all puzzle constraints.\"\"\"\n",
        "\n",
        "        normalized = self._normalize_word(word)\n",
        "        if len(normalized) < self._min_word_length:\n",
        "            return False\n",
        "        if not self.contains_only_allowed_letters(normalized):\n",
        "            return False\n",
        "        if self._required_letter not in normalized:\n",
        "            return False\n",
        "        return normalized in self._valid_words\n",
        "\n",
        "    def is_pangram(self, word: str) -> bool:\n",
        "        \"\"\"Return ``True`` if *word* uses every provided letter at least once.\"\"\"\n",
        "\n",
        "        normalized = self._normalize_word(word)\n",
        "        return self._letter_set.issubset(set(normalized))\n",
        "\n",
        "    def score_word(self, word: str) -> int:\n",
        "        \"\"\"Compute the official Spelling Bee score for *word*.\n",
        "\n",
        "        Rules (as of 2025):\n",
        "                * 4-letter words are worth 1 point.\n",
        "                * Words with length > 4 are worth their length.\n",
        "                * Pangrams receive a 7-point bonus on top of their length score.\n",
        "        \"\"\"\n",
        "\n",
        "        normalized = self._normalize_word(word)\n",
        "        if not self.is_valid_word(normalized):\n",
        "            raise ValueError(f\"Word is not valid in this puzzle: {word}\")\n",
        "\n",
        "        base = 1 if len(normalized) == 4 else len(normalized)\n",
        "        return base + (7 if self.is_pangram(normalized) else 0)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Internal helpers\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def _is_candidate_word(self, word: str) -> bool:\n",
        "        normalized = self._normalize_word(word)\n",
        "        if len(normalized) < self._min_word_length:\n",
        "            return False\n",
        "        if not normalized.isalpha():\n",
        "            return False\n",
        "        if self._required_letter not in normalized:\n",
        "            return False\n",
        "        if not set(normalized).issubset(self._letter_set):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_letter(letter: str) -> str:\n",
        "        if not letter or len(letter) != 1 or not letter.isalpha():\n",
        "            raise ValueError(f\"Invalid letter: {letter!r}\")\n",
        "        return letter.upper()\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_word(word: str) -> str:\n",
        "        return str(word or \"\").strip().upper()\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_prefix_set(words: Iterable[str]) -> Set[str]:\n",
        "        prefixes: Set[str] = set()\n",
        "        for word in words:\n",
        "            for end in range(1, len(word) + 1):\n",
        "                prefixes.add(word[:end])\n",
        "        return prefixes\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Representations\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def __repr__(self) -> str:  # pragma: no cover - human-friendly representation\n",
        "        letters = \"\".join(self._letters)\n",
        "        return (\n",
        "            f\"SpellingBeeProblem(letters='{letters}', required='{self._required_letter}', \"\n",
        "            f\"words={len(self._valid_words)})\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Generalized search implementation\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "StrategyFn = Callable[[SearchNode], float]\n",
        "\n",
        "\n",
        "async def generalized_search(\n",
        "    *,\n",
        "    problem: \"SpellingBeeProblem\",\n",
        "    cost_fn: Callable[[str, str, str], float],\n",
        "    heuristic_fn: Union[Callable[[str], float], Callable[[str], asyncio.coroutine]],\n",
        "    strategy: str = \"a_star\",\n",
        "    max_expansions: Optional[int] = None,\n",
        "    verbose: bool = False,\n",
        ") -> SearchResult:\n",
        "    \"\"\"Generic best-first search over ``SpellingBeeProblem`` states.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    problem:\n",
        "        Instance that provides ``initial_state()``, ``successors(state)`` and\n",
        "        ``is_goal(state)``.\n",
        "    cost_fn:\n",
        "        Callable returning the incremental cost ``g(parent_state, action, child_state)``.\n",
        "    heuristic_fn:\n",
        "        Callable or async callable returning an admissible heuristic estimate ``h(state)``.\n",
        "        Can be either a synchronous function or an async function.\n",
        "    strategy:\n",
        "        ``\"a_star\"`` (default) uses ``f = g + h``.  ``\"uniform_cost\"`` ignores the\n",
        "        heuristic and behaves like UCS.\n",
        "    max_expansions:\n",
        "        Optional safety limit to prevent unbounded work.\n",
        "    \"\"\"\n",
        "\n",
        "    strategy = strategy.lower()\n",
        "    if strategy not in {\"a_star\", \"uniform_cost\"}:\n",
        "        raise ValueError(\"strategy must be 'a_star' or 'uniform_cost'\")\n",
        "\n",
        "    initial_state = problem.initial_state()\n",
        "\n",
        "    # Check if heuristic_fn is async\n",
        "    is_async_heuristic = inspect.iscoroutinefunction(heuristic_fn)\n",
        "\n",
        "    try:\n",
        "        if strategy == \"a_star\":\n",
        "            if is_async_heuristic:\n",
        "                initial_heuristic = float(await heuristic_fn(initial_state))\n",
        "            else:\n",
        "                initial_heuristic = float(heuristic_fn(initial_state))\n",
        "        else:\n",
        "            initial_heuristic = 0.0\n",
        "    except Exception as exc:  # pragma: no cover - propagate context\n",
        "        raise RuntimeError(\n",
        "            \"heuristic_fn raised an exception for the initial state\"\n",
        "        ) from exc\n",
        "\n",
        "    root = SearchNode(\n",
        "        state=initial_state,\n",
        "        parent=None,\n",
        "        action=None,\n",
        "        path_cost=0.0,\n",
        "        heuristic=initial_heuristic,\n",
        "        metadata=None,\n",
        "    )\n",
        "\n",
        "    frontier: List[Tuple[float, int, SearchNode]] = []\n",
        "    counter = count()\n",
        "    heapq.heappush(frontier, (root.total_cost(), next(counter), root))\n",
        "\n",
        "    best_costs: Dict[str, float] = {initial_state: 0.0}\n",
        "    explored: Set[str] = set()\n",
        "    expansions = 0\n",
        "\n",
        "    while frontier:\n",
        "        if max_expansions is not None and expansions >= max_expansions:\n",
        "            break\n",
        "\n",
        "        _, _, node = heapq.heappop(frontier)\n",
        "\n",
        "        if problem.is_goal(node.state):\n",
        "            return _build_search_result(\n",
        "                node,\n",
        "                success=True,\n",
        "                expansions=expansions,\n",
        "                explored=len(explored),\n",
        "                frontier_size=len(frontier),\n",
        "            )\n",
        "\n",
        "        if node.state in explored:\n",
        "            continue\n",
        "\n",
        "        explored.add(node.state)\n",
        "        expansions += 1\n",
        "\n",
        "        for action, next_state, _metadata in problem.successors(node.state):\n",
        "            step_cost = float(cost_fn(node.state, action, next_state))\n",
        "            if step_cost < 0:\n",
        "                raise ValueError(\"cost_fn must return non-negative values\")\n",
        "\n",
        "            new_cost = node.path_cost + step_cost\n",
        "            if next_state in best_costs and new_cost >= best_costs[next_state]:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                if strategy == \"a_star\":\n",
        "                    if is_async_heuristic:\n",
        "                        heuristic_value = float(await heuristic_fn(next_state))\n",
        "                    else:\n",
        "                        heuristic_value = float(heuristic_fn(next_state))\n",
        "                else:\n",
        "                    heuristic_value = 0.0\n",
        "            except Exception as exc:\n",
        "                raise RuntimeError(\n",
        "                    f\"heuristic_fn raised an exception for state {next_state!r}\"\n",
        "                ) from exc\n",
        "\n",
        "            if verbose:\n",
        "                if len(next_state) > 4:\n",
        "                    print(f\"Expanding: {node.state!r} + {action!r} -> {next_state!r}\")\n",
        "                    print(f\"  Step cost: {step_cost}\")\n",
        "                    print(f\"  New cost: {new_cost}\")\n",
        "                    print(f\"  Heuristic score: {heuristic_value}\")\n",
        "\n",
        "            child = SearchNode(\n",
        "                state=next_state,\n",
        "                parent=node,\n",
        "                action=action,\n",
        "                path_cost=new_cost,\n",
        "                heuristic=heuristic_value,\n",
        "                metadata=_metadata,\n",
        "            )\n",
        "\n",
        "            best_costs[next_state] = new_cost\n",
        "            heapq.heappush(frontier, (child.total_cost(), next(counter), child))\n",
        "\n",
        "    return _build_search_result(\n",
        "        node=None,\n",
        "        success=False,\n",
        "        expansions=expansions,\n",
        "        explored=len(explored),\n",
        "        frontier_size=len(frontier),\n",
        "    )\n",
        "\n",
        "\n",
        "def _build_search_result(\n",
        "    node: Optional[SearchNode],\n",
        "    *,\n",
        "    success: bool,\n",
        "    expansions: int,\n",
        "    explored: int,\n",
        "    frontier_size: int,\n",
        ") -> SearchResult:\n",
        "    if success and node is None:\n",
        "        raise ValueError(\"Successful search must supply a goal node\")\n",
        "\n",
        "    actions: List[str] = []\n",
        "    cost = float(\"inf\")\n",
        "    goal_state: Optional[str] = None\n",
        "\n",
        "    if node is not None:\n",
        "        cost = node.path_cost\n",
        "        goal_state = node.state\n",
        "        actions = _reconstruct_actions(node)\n",
        "\n",
        "    return SearchResult(\n",
        "        success=success,\n",
        "        goal_state=goal_state,\n",
        "        actions=actions,\n",
        "        cost=cost if success else float(\"inf\"),\n",
        "        expansions=expansions,\n",
        "        explored=explored,\n",
        "        frontier_size=frontier_size,\n",
        "    )\n",
        "\n",
        "\n",
        "def _reconstruct_actions(node: SearchNode) -> List[str]:\n",
        "    actions: List[str] = []\n",
        "    cursor: Optional[SearchNode] = node\n",
        "    while cursor and cursor.action is not None:\n",
        "        actions.append(cursor.action)\n",
        "        cursor = cursor.parent\n",
        "    actions.reverse()\n",
        "    return actions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4WxhXBXXimW",
        "outputId": "5ca39894-5be6-4d23-8006-eead06e78870"
      },
      "id": "T4WxhXBXXimW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0c2d75f5",
      "metadata": {
        "id": "0c2d75f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8933b1d3-bf16-4def-f477-91db34256a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autogen-core in /usr/local/lib/python3.12/dist-packages (0.7.5)\n",
            "Requirement already satisfied: autogen-agentchat in /usr/local/lib/python3.12/dist-packages (0.7.5)\n",
            "Requirement already satisfied: autogen-ext[azure,openai] in /usr/local/lib/python3.12/dist-packages (0.7.5)\n",
            "Requirement already satisfied: jsonref~=1.1.0 in /usr/local/lib/python3.12/dist-packages (from autogen-core) (1.1.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from autogen-core) (1.37.0)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.12/dist-packages (from autogen-core) (11.3.0)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.12/dist-packages (from autogen-core) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from autogen-core) (2.11.9)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from autogen-core) (4.15.0)\n",
            "Requirement already satisfied: azure-ai-inference>=1.0.0b9 in /usr/local/lib/python3.12/dist-packages (from autogen-ext[azure,openai]) (1.0.0b9)\n",
            "Requirement already satisfied: azure-ai-projects>=1.0.0b11 in /usr/local/lib/python3.12/dist-packages (from autogen-ext[azure,openai]) (1.1.0b4)\n",
            "Requirement already satisfied: azure-core in /usr/local/lib/python3.12/dist-packages (from autogen-ext[azure,openai]) (1.35.1)\n",
            "Requirement already satisfied: azure-identity in /usr/local/lib/python3.12/dist-packages (from autogen-ext[azure,openai]) (1.25.1)\n",
            "Requirement already satisfied: azure-search-documents>=11.4.0 in /usr/local/lib/python3.12/dist-packages (from autogen-ext[azure,openai]) (11.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from autogen-ext[azure,openai]) (24.1.0)\n",
            "Requirement already satisfied: openai>=1.93 in /usr/local/lib/python3.12/dist-packages (from autogen-ext[azure,openai]) (1.109.1)\n",
            "Requirement already satisfied: tiktoken>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from autogen-ext[azure,openai]) (0.11.0)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from azure-ai-inference>=1.0.0b9->autogen-ext[azure,openai]) (0.7.2)\n",
            "Requirement already satisfied: azure-storage-blob>=12.15.0 in /usr/local/lib/python3.12/dist-packages (from azure-ai-projects>=1.0.0b11->autogen-ext[azure,openai]) (12.26.0)\n",
            "Requirement already satisfied: azure-ai-agents>=1.2.0b3 in /usr/local/lib/python3.12/dist-packages (from azure-ai-projects>=1.0.0b11->autogen-ext[azure,openai]) (1.2.0b5)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from azure-core->autogen-ext[azure,openai]) (2.32.4)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from azure-core->autogen-ext[azure,openai]) (1.17.0)\n",
            "Requirement already satisfied: azure-common>=1.1 in /usr/local/lib/python3.12/dist-packages (from azure-search-documents>=11.4.0->autogen-ext[azure,openai]) (1.1.28)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.93->autogen-ext[azure,openai]) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.93->autogen-ext[azure,openai]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.93->autogen-ext[azure,openai]) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.93->autogen-ext[azure,openai]) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.93->autogen-ext[azure,openai]) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=1.93->autogen-ext[azure,openai]) (4.67.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.34.1->autogen-core) (8.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.8.0->autogen-ext[azure,openai]) (2024.11.6)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.12/dist-packages (from azure-identity->autogen-ext[azure,openai]) (43.0.3)\n",
            "Requirement already satisfied: msal>=1.30.0 in /usr/local/lib/python3.12/dist-packages (from azure-identity->autogen-ext[azure,openai]) (1.34.0)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from azure-identity->autogen-ext[azure,openai]) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.93->autogen-ext[azure,openai]) (3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.5->azure-identity->autogen-ext[azure,openai]) (2.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.93->autogen-ext[azure,openai]) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.93->autogen-ext[azure,openai]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.93->autogen-ext[azure,openai]) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core) (3.23.0)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->autogen-ext[azure,openai]) (2.10.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core->autogen-ext[azure,openai]) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core->autogen-ext[azure,openai]) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity->autogen-ext[azure,openai]) (2.23)\n"
          ]
        }
      ],
      "source": [
        "%pip install \"autogen-core\" \"autogen-agentchat\" \"autogen-ext[openai,azure]\"\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0678a3ac",
      "metadata": {
        "id": "0678a3ac"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "from dataclasses import asdict\n",
        "\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.conditions import TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.base import TaskResult\n",
        "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
        "\n",
        "from utils import SpellingBeeProblem, SearchResult, generalized_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "61e72813",
      "metadata": {
        "id": "61e72813"
      },
      "outputs": [],
      "source": [
        "# Just like in the other labs\n",
        "azure_deployment = \"gpt-5-mini-lab1\"\n",
        "api_version = \"2024-12-01-preview\"\n",
        "azure_endpoint = \"https://lab1agent-aifoundry.cognitiveservices.azure.com/\"\n",
        "\n",
        "# Expect AZURE_SUBSCRIPTION_KEY to be set in environment variables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2575983f",
      "metadata": {
        "id": "2575983f"
      },
      "source": [
        "## Agentic Heuristic Architecture Blueprint\n",
        "\n",
        "Based on the overview, sketch out how your heuristic agents and the generalized search tool interact:\n",
        "- **Feasibility Agent**: Evaluates constraint satisfaction and dictionary viability.\n",
        "- **Completeness Agent**: Estimates remaining effort to reach a valid Spelling Bee solution.\n",
        "- **Score Aggregator Agent**: Combines the scores and analysis of other agents into a final score.\n",
        "- *(Optional)* Additional agents for scoring letter diversity, pangram potential, etc.\n",
        "\n",
        "NOTE: The below system is just a example. Feel free to make it your own as you designed in part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2b5946f6",
      "metadata": {
        "id": "2b5946f6"
      },
      "outputs": [],
      "source": [
        "def setup_agentic_heuristic_system():\n",
        "    \"\"\"Instantiate heuristic agents for Spelling Bee state evaluation.\"\"\"\n",
        "    client = AzureOpenAIChatCompletionClient(\n",
        "        azure_deployment=azure_deployment,\n",
        "        model=\"gpt-5-mini\",\n",
        "        api_version=api_version,\n",
        "        azure_endpoint=azure_endpoint,\n",
        "        # api_key=os.getenv(\"AZURE_SUBSCRIPTION_KEY\"),\n",
        "        api_key = userdata.get('api_key')\n",
        "    )\n",
        "\n",
        "    scoring = AssistantAgent(\n",
        "        name=\"ScoringAgent\",\n",
        "        model_client=client,\n",
        "        system_message=\"\"\"Generate a score based on the existing NYT rules:\n",
        "        1. Four-letter words are 1 point\n",
        "        2. 1 point for each letter after four letters\n",
        "        3. Add 7 bonus points if the word uses all 7 letters in one word\n",
        "        End responses with ScoringAgent: <float>\"\"\",\n",
        "    )\n",
        "\n",
        "    completion_effort = AssistantAgent(\n",
        "        name=\"CompletionEffortAgent\",\n",
        "        model_client=client,\n",
        "        system_message=\"\"\"Determine the effort to create a complete word from\n",
        "        the partial word provided. Higher score is better here, lower score\n",
        "        is worse. End responses with CompletionEffortAgent: <float>\"\"\",\n",
        "    )\n",
        "\n",
        "    likely_to_form = AssistantAgent(\n",
        "        name=\"LikelyToFormAgent\",\n",
        "        model_client=client,\n",
        "        system_message=\"\"\"Determine likelihood that a valid word\n",
        "        (or multiple valid words) can be formed from the partial word and\n",
        "        generate a score. Higher score is better, lower score is owrse.\n",
        "        End responses with LikelyToFormAgent: <float>\"\"\",\n",
        "    )\n",
        "\n",
        "    score_combiner = AssistantAgent(\n",
        "        name=\"ScoreCombiner\",\n",
        "        model_client=client,\n",
        "        system_message=\"\"\"Generate a final score using the scores from the\n",
        "        three other agents. Higher is better, and lower is worse here. End\n",
        "        response with ScoreCombiner: <float>\"\"\",\n",
        "    )\n",
        "\n",
        "    return scoring, completion_effort, likely_to_form, score_combiner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8d32132e",
      "metadata": {
        "id": "8d32132e"
      },
      "outputs": [],
      "source": [
        "def cost_fn(parent_state, action, next_state) -> float:\n",
        "    \"\"\"\n",
        "    Return the incremental cost g(n) for moving to next_state.\n",
        "    Customize this to reflect letter usage, word length, or other criteria.\n",
        "\n",
        "    Args:\n",
        "        parent_state: The current sub-word.\n",
        "        action: The next letter added.\n",
        "        next_state: The sub-word after adding the action letter.\n",
        "    Returns:\n",
        "        A numeric cost value (float).\n",
        "    \"\"\"\n",
        "    # Simple example: cost is proportional to the length of the new state\n",
        "    return len(next_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "562f3961",
      "metadata": {
        "id": "562f3961"
      },
      "outputs": [],
      "source": [
        "async def run_agentic_search(\n",
        "    spelling_bee: SpellingBeeProblem, strategy: str = \"a_star\"\n",
        "):\n",
        "    \"\"\"Execute the generalized search with your cost and heuristic functions.\"\"\"\n",
        "    scoring, completion_effort, likely_to_form, score_combiner = setup_agentic_heuristic_system()\n",
        "\n",
        "    async def heuristic_fn(state, min_state_len=3) -> float:\n",
        "        \"\"\"\n",
        "        Delegate to the agentic heuristic system to compute h(n) for `state`. In our problem space,\n",
        "        state is a partial word. You will need to implement the logic to send messages to your agents,\n",
        "        gather their responses, and compute a final numeric heuristic value.\n",
        "\n",
        "        Args:\n",
        "            state: The current state (partial word) to evaluate.\n",
        "        Returns:\n",
        "            A numeric heuristic estimate (float).\n",
        "        \"\"\"\n",
        "        if len(state) < min_state_len:\n",
        "            # No heuristic value for very short states\n",
        "            # There isn't enough information to evaluate\n",
        "            return 0.0\n",
        "\n",
        "        # See https://microsoft.github.io/autogen/stable//reference/python/autogen_agentchat.teams.html\n",
        "        team = RoundRobinGroupChat(\n",
        "            [scoring, completion_effort, likely_to_form, score_combiner],\n",
        "            termination_condition=TextMentionTermination(\"FINAL_SCORE:\"),\n",
        "        )\n",
        "\n",
        "        # Get the result from the team\n",
        "        messages_generator = team.run_stream(\n",
        "            task=f\"Evaluate heuristic for state: {state}\", output_task_messages=False\n",
        "        )\n",
        "        async for message in messages_generator:\n",
        "            if isinstance(message, TaskResult):\n",
        "                response = message.messages[-1].content\n",
        "                break\n",
        "\n",
        "        # Parse the numeric score from the orchestrator's response\n",
        "        try:\n",
        "            score_line = next(\n",
        "                line for line in response.splitlines() if \"FINAL_SCORE:\" in line\n",
        "            )\n",
        "            start_idx = score_line.index(\"FINAL_SCORE:\") + len(\"FINAL_SCORE:\")\n",
        "            end_idx = score_line.index(\".\", start_idx) + 1\n",
        "            final_score = float(score_line[start_idx:end_idx].strip())\n",
        "            return final_score\n",
        "        except (StopIteration, ValueError, IndexError):\n",
        "            print(\"Failed to parse score from orchestrator response. Defaulting to 0.0\")\n",
        "            return 0.0\n",
        "\n",
        "    print(f\"Running {strategy} search on: {spelling_bee}\")\n",
        "\n",
        "    result: SearchResult = await generalized_search(\n",
        "        problem=spelling_bee,\n",
        "        cost_fn=cost_fn,\n",
        "        heuristic_fn=heuristic_fn,\n",
        "        strategy=strategy,\n",
        "        max_expansions=None,  # May want to set this for debugging\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    result_summary = asdict(result)\n",
        "    print(\"SearchResult summary:\")\n",
        "    for key, value in result_summary.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e27464",
      "metadata": {
        "id": "11e27464"
      },
      "source": [
        "## Example Puzzle 1: Starter Configuration\n",
        "\n",
        "Test the integrated system on a small Spelling Bee instance provided with the lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7224610e",
      "metadata": {
        "id": "7224610e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957d6d60-a8e5-4801-867e-0fa0dbdbae78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running a_star search on: SpellingBeeProblem(letters='ADELOPR', required='O', words=9)\n"
          ]
        }
      ],
      "source": [
        "# Provided helper will create a Spelling Bee problem instance, e.g. letters=\"ADELOPR\", center=\"O\"\n",
        "starter_problem = SpellingBeeProblem.from_letters(\n",
        "    letters=[\"A\", \"D\", \"E\", \"L\", \"O\", \"P\", \"R\"],\n",
        "    required_letter=\"O\",\n",
        ")\n",
        "\n",
        "# Uncomment to run once cost_fn and heuristic_fn are implemented\n",
        "await run_agentic_search(starter_problem, strategy=\"a_star\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c09db3e",
      "metadata": {
        "id": "5c09db3e"
      },
      "source": [
        "## Example Puzzle 2: Alternate Strategy Comparison\n",
        "\n",
        "Run the same instance under Uniform Cost Search to compare behavior vs. A*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62112e4",
      "metadata": {
        "id": "b62112e4"
      },
      "outputs": [],
      "source": [
        "# Uncomment to compare strategies once heuristic_fn is operational\n",
        "# await run_agentic_search(starter_problem, strategy=\"uniform_cost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b502b40b",
      "metadata": {
        "id": "b502b40b"
      },
      "source": [
        "## Your Experiment\n",
        "\n",
        "Define your own Spelling Bee instance or heuristic variant and record results. This could be [today's puzzle](https://www.nytimes.com/puzzles/spelling-bee)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcc3672b",
      "metadata": {
        "id": "dcc3672b"
      },
      "outputs": [],
      "source": [
        "custom_problem = SpellingBeeProblem.from_letters(\n",
        "    letters=[...],\n",
        "    required_letter=\"...\",\n",
        ")\n",
        "\n",
        "# await run_agentic_search(custom_problem, strategy=\"a_star\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50c59100",
      "metadata": {
        "id": "50c59100"
      },
      "source": [
        "## Reflection & Analysis\n",
        "\n",
        "### Heuristic effectiveness\n",
        "The heuristic was pretty ineffective as it ran for 25 minutes with no results.\n",
        "It seems like a completely different strategy is necessary to make it work.\n",
        "\n",
        "### Failure modes / surprises\n",
        "I didn't expect it to take so long. I think the way the heuristic works\n",
        "caused it to take way too long on each state, and the amount of states\n",
        "exponentially increased due to the increasing fringe.\n",
        "\n",
        "### Cost vs. heuristic alignment\n",
        "Cost here seems like it's a bit more basic and runs on a different scale.\n",
        "I think functionally they do similar things which is figuring out the cost of various decisions. Cost is also probably better because it doesn't take nearly as much time as an agentic system.\n",
        "\n",
        "### Communication insights\n",
        "The system didn't show any messages, which definitely hindered collaboration and my understanding of how the system was doing.\n",
        "\n",
        "### Future improvements\n",
        "I think the agents could probably run once at the beginning instead of at every state like it currently does. It would decrease the time complexity significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1d173ae",
      "metadata": {
        "id": "a1d173ae"
      },
      "source": [
        "## References\n",
        "\n",
        "- `L3_overview.md`\n",
        "- [AutoGen Documentation](https://microsoft.github.io/autogen/stable/index.html)\n",
        "- [Model Context Protocol](https://modelcontextprotocol.io/docs/getting-started/intro)\n",
        "- [Agent-to-Agent Protocol](https://a2a-protocol.org/latest/)\n",
        "- [NYT Spelling Bee](https://www.nytimes.com/puzzles/spelling-bee)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}